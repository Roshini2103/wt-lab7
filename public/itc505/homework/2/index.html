<!DOCTYPE html>
<html>

<head>
    <!--addedum is written for each of them in comments-->
    <!--Creating a title tag that informs users of the page they are on -->
    <title>Homework 2</title>
    <!--Making a meta tag to tell the framework that the framework ought to utilize the UTF-8 which is an encoding framework for Unicode-->
    <meta charset="UTF-8">
</head>

<body>
    <!--Making an article tag to address the article beginning for the very much planned research paper-->
    <article>
        <!--Making a heading tag with h1 to display the article's heading displays the heights of all header tags-->
        <h1>
            <!--Focuses every one of the items under this tag to have a uniform an efficient construction and simple to distinguish-->
            <center>
                <b>
                    <!--to specify the bold text without any extra importance -->
                    AN INSIGHT INTO EDGE ARTIFICIAL INTELLIGENCE
                </b>
            </center>
        </h1>
        <!--Header tag with the third level of all current tag-->
        <h3>
            <center>
                Adithya Sriramoju , G. Sujith , Mdn Akash
            </center>
        </h3>
        <h3>
            <center>
                Department Of Computer Science And Engineering, Vardhaman College Of Engineering,
                Hyderabad, Telangana, India
            </center>
        </h3>
        <!--Adding a section to the below abstract to set it apart from the rest of the existing ones-->
        <section>
            <!--creating striking elements that distinguish the content from the other elements-->
            <b>
                ABSTRACT
            </b>
            <!--Making a passage to put under all the substance under the segment heading-->
            <p>
                Artificial Intelligence at the edge can be simply said like, having data in a local area, where it can
                be accessed
                quickly rather than having it in the cloud which causes latency. For example, in a self-driving, where
                there is an
                absolute need for retrieving instructions as quickly as possible to avoid any major accident. Here, AI
                at the edge
                can play a vital role. This paper discusses the overviews, features as well as challenges that will be
                faced in the
                implementation of AI at the edge
                <br>
                <br>
                <b>Keywords:</b> : Edge Artificial Intelligence, Machine Learning, Deep Learning, Edge, Cloud.
            </p>
        </section>
        <!--Making a request rundown of segment to have the option to recognize different pieces of the paper-->
        <ol>
            <!--Creating one of the elements on the list for each element below it-->
            <li>
                <b>
                    INTRODUCTION
                </b>
                <p>
                    Unlike most of you, who are enthralled by artificial intelligence, consider this: instead of you
                    doing all of your
                    jobs, you get a machine to do everything for you, but it can also do something you thought was
                    impossible, such
                    as predict the future, such as earthquakes and tsunamis, so that preventive steps can be taken to
                    save lives.
                    Chatbots, artificial assistant assistants such as Siri on iPhones, Google Home, and, believe it or
                    not, it is becoming
                    smarter by the day. Self-driving vehicles with machine learning will be a blessing for old and
                    disabled persons
                    who makes it challenging to drive on their own, and they will also save many accidents caused by
                    human
                    mistake. The ability of a machine to simulate intelligent human behavior is known as artificial
                    intelligence.
                    Knowing how a human mind thinks, learns, and operates while seeking to solve a problem is how AI is
                    developed. Until now, artificial intelligence, machine learning, and deep learning have all been
                    buzzwords. Do
                    you, on the other hand, have any idea how they're related? Deep learning, as a subfield of
                    artificial intelligence,
                    is a subfield of machine learning, which is itself a subfield of AI. It's tempting to think of
                    AlphaGo as a huge
                    accomplishment for deep learning, but it's actually a mash-up of ideas from several different
                    domains of AI and
                    machine learning. Deep neural networks are a well-established idea that dates back to the 1950s.
                    However, we
                    won't be able to implement it until we have the new high-end resource capabilities. Let's examine
                    machine
                    learning and its limitations. Machine learning is an artificial intelligence subset that allows
                    computers to learn
                    without having to be explicitly programmed. Unlike other programming applications, we do not have to
                    declare
                    all the steps of conditions in machine learning. However, we must train the machine on a large
                    enough training
                    data set to develop a model that aids the machine in making judgments based on its learning. For
                    example, if we
                    want to use a machine to determine the species of a flower, we must first educate the machine using
                    a floral
                    data set that includes numerous features of different flowers as well as their corresponding
                    species. Let's
                    pretend we know the sepal length, sepal breadth, teittleman's petal width, and the flower's species.
                    As a result
                    of this input data set, a computer was constructed that can classify flowers. However, there are
                    certain
                    drawbacks to this method. The curse of dimensionality refers to the inability of machine learning to
                    handle
                    high-dimensional data, which is defined as data that is enormous in size and has several dimensions.
                    Handling
                    and analyzing such data become extremely difficult. So, to put it another way, consider a
                    hundred-yard line and
                    the assumption that you drop the coin anywhere down the line. Simply walking on the line aligned in
                    a onedimension entity will lead you to the coin. Consider the following scenario: you have a square
                    with 100 yards
                    on each side, and you drop the coin somewhere within the square. It will now take you longer to
                    locate the coin
                    within that square. A two-dimensional thing is a square. Let's take it a step further and imagine a
                    cube with
                    each side measuring 500 yards. If you drop the penny somewhere inside the cube, finding it becomes
                    even
                    more difficult. As we can see, complexity rises as the number of dimensions rises, and in real life,
                    the highdimensional data we're talking about has a lot of dimensions, making it difficult to handle
                    and process. Picture
                    processing, natural language processing, image translation, and other applications can quickly find
                    highdimensional data. Machine learning was unable to solve the few problems, thus deep learning was
                    called upon
                    to help. Deep learning can handle large amounts of data and is also capable of focusing on the
                    relevant features
                    on its own, a process known as feature extraction. Let's have a look at how deep learning is
                    currently
                    implemented. A neuron, artificial neuron, or perceptron was produced in an attempt to re-engineer a
                    human
                    brain, and deep learning explores the basic unit of the brain known as a brain cell or Anula. When
                    you examine
                    the structure of a biological neuron, you'll notice that it has dendrites, which are used to accept
                    inputs before
                    being summed up inside the cell body and sent on to the next biological neuron through the axon.
                    Similarly, a
                    perceptron receives a large number of inputs, performs numerous transformations and functions, and
                    then
                    outputs. A deep neural network is made up of a network of artificial neurons known as a perceptron,
                    much like
                    our brain is made up of numerous connected neurons known as a neural network. Let's have a look at
                    the
                    anatomy of a deep neural network. Input layers, hidden layers, and output layers are the three tiers
                    of a deep
                    neural network. As you can see in the diagram, the first layer is the input layer, which also
                    receives all of the
                    inputs, and the last layer is the output layer, which provides the desired result, and all of the
                    layers in between
                    are called hidden layers; there can be n number of hidden layers, especially with the high resources
                    available
                    these days, and the number of hidden layers and perceptron's for each share will be entirely
                    dependent on the
                    use case that you are working on. Let's take a high-level look at how a deep neural network
                    approaches a
                    problem. We'll need to send this high-dimensional data to the input layer and mask the
                    dimensionality of the
                    input data if we wish to use a deep network for picture recognition. The input layer will have
                    multiple sublayers of perceptron’s to take over the entire input, and the outcome from the input
                    layer will comprise
                    patterns and would only be able to identify edges and sensitive on contrast levels. This outcome is
                    sent to
                    hidden layer 1, which would be able to identify the different features such as eyes, nose, ears, and
                    etc. This will
                    now be delivered to hidden layer 2 where it will be able to create the whole faces before being sent
                    to the
                    output layers to be categorized and named. Consider whether any of these layers are missing or
                    whether the
                    neural network is too shallow. So, what happens next? Simply said, we won't be able to reliably
                    identify the
                    photographs, which is why these new instances remained unsolved for the entire year prior to deep
                    learning.
                    As a result, we may deduce that deep learning influenced the outcomes of many AI applications and
                    gave a
                    solution to problems that AI and machine learning couldn't solve. Edge Artificial Intelligence
                    refers to the use of
                    edge computing to run AI algorithms locally on a physical device, without the need for a network
                    connection.
                    This allows you to evaluate data on the device in milliseconds or less, giving you real-time
                    information. We'll
                    have to find out what problem we'll investigate. Consider performance optimization: the optimization
                    goal,
                    decision variables, and potential constraints all need to be checked [1]. Artificial intelligence on
                    the edge has a
                    wide range of applications, including natural language processing [2].
                    <center>
                        <img src="/itc505/homework/2/images/image 1.png">
                    </center>
                    <br>
                    <br>
                    The diagram above also serves as an excellent illustration of the importance of edge artificial
                    intelligence in
                    edge computing. Because means for implementing any system are significant, the number of edge
                    devices that
                    are now in use plays a critical role in developing edge AI because as resources for execution
                    increase, the scope
                    for edge artificial intelligence expands [3].
                    <br>
                    <br>
                </p>
            </li>
            <li>
                <b>
                    CHALLENGES OF EDGE ARTIFICIAL INTELLIGENCE
                </b>
                <br>
                <br>
                <!--Creating a list of elements with starting with A,B,C as a list.-->
                <ol type="A">
                    <li>
                        Artificial Intelligence at the edge will never achieve its rightful place among emerging
                        technologies without
                        confidence and a robust set of security measures. On the one hand, data processing locally has
                        inherent
                        advantages because the data remains in the targeted sovereign region rather than traveling via
                        the network
                        to the core [4].
                    </li>
                    <li>
                        On the other hand, maintaining data locally implies protecting and securing multiple locations
                        at the same time, with more physical access allowing for various types of urgent threats.
                        Although it appears that
                        innovation occurs at the edge, where people engage with technology, the key to successful
                        implementation
                        is balancing the workload between the locals and the cloud. The promise of Artificial
                        Intelligence at the edge
                        will be fully shown if we get these foundations correctly [4].
                    </li>
                    <li>
                        Machine learning and deep learning techniques, which are the most significant in Artificial
                        Intelligence,
                        necessitate a large number of calculations to be performed quickly. This implies that they
                        consume a
                        significant amount of computing power [4].
                    </li>
                    <li>
                        Cloud computing and massively parallel processing systems have provided the solution in the
                        short term. As
                        data volumes grow and deep learning supports the automated building of increasingly complex
                        algorithms,
                        the bottleneck will continue to stymie advancement [4]. There are still insufficient humans to
                        allow each
                        company or organization to fully realize its vision of how the world is advancing thanks to
                        machine power.
                        There is a skills shortage, just as there is in other domains of science and technology – there
                        aren't enough
                        individuals who know how to operate self-learning machines [4].
                    </li>
                </ol>
                <br>
                <br>
            </li>
            <li>
                <b>
                    FEATURES OF EDGE ARTIFICIAL INTELLIGENCE
                </b>
                <br>
                <br>
                <ol type="A">
                    <li>
                        Reduced Costs
                        <br>
                        With Edge Artificial Intelligence, data connectivity and bandwidth costs will be reduced because
                        fewer data
                        would be transported. AI processing on the cloud is also significantly more expensive because of
                        the high cost
                        of AI device hardware [5].
                    </li>
                    <li>
                        Security
                        <br>
                        When it comes to integrating Artificial Intelligence in settings like security cameras,
                        driverless cars, drones, and
                        so on, data is a key concern for customers. Because Edge Artificial Intelligence processes data
                        locally, you may
                        avoid the issue of streaming without uploading a large amount of data to the cloud, which leaves
                        you exposed in
                        terms of privacy. Edge Artificial Intelligence has a processing speed of only a few
                        milliseconds, which drastically
                        reduces the risk of data manipulation while in transit. Furthermore, enhanced security methods
                        can be added
                        to Edge Artificial Intelligence devices, making them even safer [5].
                    </li>
                    <li>
                        Highly Responsive
                        <br>
                        Edge AI devices can process data more quickly than centralized IoT models. Because insights are
                        processed in
                        real-time within the same hardware, they enable real-time processes such as data production,
                        decision making,
                        and action, making them ideal for applications where microseconds matter, such as self-driving
                        cars [5].
                    </li>
                    <li>
                        Edge is simple to use
                        <br>
                        Artificial Intelligence devices are self-contained and do not require the assistance of data
                        scientists or AI
                        developers to stay current. Data and insights are either provided automatically or made
                        available on-demand
                        via highly graphic or dashboard interfaces [5].
                    </li>
                </ol>
                <br>
                <br>
            </li>
            <li>
                <b>
                    APPLICATIONS OF EDGE ARTIFICIAL INTELLIGENCE
                </b>
                <br>
                <br>
                <ol type="A">
                    <li>
                        <b>Surveillance and Monitoring purposes</b>
                        <br>
                        <br>
                        Using Edge AI, smart cameras with machine learning capabilities can now analyse captured images
                        locally to
                        identify and monitor a variety of goods and people, as well as detect suspicious behaviour.
                        Camera footage will
                        never have to go to the cloud server except for triggering events, conserving bandwidth [5][6].
                        This means that
                        the greater the number, the better.
                        The server can communicate with a large number of cameras with ease, reducing remote processor
                        and storage
                        utilization [5][6].
                    </li>
                    <br>
                    <li>
                        <b>Autonomous Vehicles</b>
                        <br>
                        <br>
                        Artificial Intelligence at the edge processes data in real-time on the same local hardware. As a
                        result, they
                        enable real-time operations, such as driverless vehicles. To function properly, an autonomous
                        vehicle requires
                        data to be processed quickly, such as detecting vehicles, traffic signs, pedestrians, roads, and
                        so on. Edge
                        Artificial Intelligence can quickly locate and process all of the information required by the
                        main controller
                        [5][6].
                    </li>
                    <br>
                    <li>
                        <b>Smart Speakers</b>
                        <br>
                        <br>
                        Edge AI allows machine learning-enabled smart cameras to interpret acquired pictures locally,
                        allowing them
                        to identify and track multiple objects and people, as well as to detect suspicious activity,
                        right at the edge. With
                        the exception of triggering events, camera footage will never have to go to the cloud server,
                        decreasing
                        bandwidth consumption [5][6]. As a result, the greater the number, the better.
                    </li>
                    <br>
                    <li>
                        <b>Industrial IoT </b>
                        <br>
                        <br>
                        Artificial intelligence will be necessary in the future to automate a factory in order to make
                        it more effective and
                        efficient, from rigorous inspection for defects through robotic assembly control. Edge
                        Artificial Intelligence
                        allows you to apply Artificial Intelligence capabilities at a cheaper cost, allowing you to
                        analyze data more
                        quickly [5][6].
                    </li>
                    <br>
                    <li>
                        <b>Digital Signals Processor</b>
                        <br>
                        <br>
                        DSPs are commonly used in AI applications. Real-time DSPs are vital in Ai technologies, which
                        include data and
                        signal processing, signal denoising, and feature extraction [7][8][9].
                    </li>
                    <br>
                    <li>
                        <b>Eco-Friendly</b>
                        <br>
                        <br>
                        While processing data locally does not make a company or product environmentally green,
                        developing an
                        effective AI device makes sense. A small to medium IoT device will send 1MB or less each day,
                        equating to
                        approximately 20g of CO2. Over the course of a year, 10,000 devices are responsible for up to 73
                        tons of CO2
                        emissions! Local processing could cut that down to 730kg, which would be far better for the
                        environment.
                        Remember that a film or image-based solution could have a considerably bigger impact [10].
                    </li>
                </ol>
                <br>
            </li>
            <li>
                <b>
                    LIMITATIONS OF EDGE ARTIFICIAL INTELLIGENCE
                </b>
                <br>
                <br>
                <ol type="A">
                    <li>
                        In order to implement AI, we need a device that can support and operate it [10].
                    </li>
                    <br>
                    <br>
                    <li>When compared to a device that is connected to the cloud and is powered by virtual servers, an
                        Edge device
                        always will perform worse [10].</li> <br>
                    <br>
                    <li>In order to have a completely trained device, it must be connected to the internet. Even while
                        interference
                        occurs quickly, training the system takes a considerable time, even on a fully functional device
                        [10].
                    </li>
                    <br>
                    <br>
                </ol>
            </li>
            <li>
                <b>
                    CONCLUSION
                </b>
                <br>
                <br>
                The Edge Artificial Intelligence has arisen as a significant application or entity to reduce latency for
                obtaining
                data as a result of the rising interconnection of humans and things, as well as the rapid development of
                edge, AI
                methodologies, and hardware. We discussed the role of edge intelligence in this study to help accelerate
                the
                development of AI-related applications. We've highlighted how edge Artificial Intelligence exacerbates
                the
                inherent difficulty of AI applications and Machine Learning. In terms of the stakes, we see a pressing
                need for
                data to be accessed promptly in order to ensure that we receive the results we want from Artificial
                Intelligence.
                Artificial Intelligence at the edge has a lot of potential in automobiles (such as autonomous vehicles).
                Artificial
                Intelligence at the edge would have a significant impact on the outcomes of other AI-based applications
                because, in any technology, speed and efficiency are critical.
                <br>
                <br>
            </li>
            <li>
                <b>
                    REFERENCES
                </b>
                <br>
                <br>
                <!--Creating a list under another exxiting list for the references-->
                <ol>
                    <li>
                        Shuiguang Deng et. al, “Edge Intelligence: The Confluence of Edge Computing and Artificial
                        Intelligence”, In. IEEE Internet Of Things Journal, Vol-2, pp. 1-13, February 2020.</li>
                    <li>
                        D. Wang, E. Nyberg, "A long short-term memory model for answer sentence selection in question
                        answering", Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural
                        Lang.
                        Process., vol. 2, pp. 707-712, 2015.</li>
                    <li>
                        https://www.imagimob.com/blog/edge-computing-needs-edge-ai, last accessed – 29/02/2021.</li>
                    <li>
                        https://techerati.com/features-hub/opinions/ai-at-the- edge-challenges-and-opportunities/, last
                        accessed - 01/03/2020.</li>
                    <li>
                        https://www.seeedstudio.com/blog/2020/01/20/what-is- edge-ai-and-what-is-it-used-for/ , last
                        accessed- 01/03/2020.</li>
                    <li>
                        https://wire19.com/what-is-edge-ai/, last accessed -03/03/2021.</li>
                    <li>
                        Emmanuel Oyekanlu et. al, “Towards Low-Cost, Real-Time, Distributed Signal and Data Processing
                        forArtificial Intelligence Applications at Edges of Large Industrial and Internet Networks”, In.
                        2018 IEEE
                        First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),
                        IEEE, pp.
                        166-167, 2018.</li>
                    <li>
                        J. Szalai, F. Mozes, “Intelligent Digital Signal and FeatureExtraction Methods”, Springer Int’l
                        Publishing,
                        2016.</li>
                    <li>
                        R. Oshana, “DSP Software Development Techniques for Embedded and Real-Time Systems”, Elsevier,
                        2006.</li>
                    <li>
                        https://www.witekio.com/fr/blog/edge-artificial- intelligence-matters/, last accessed –
                        03/03/2021.
                    </li>
                </ol>
            </li>
        </ol>
    </article>
</body>

</html>